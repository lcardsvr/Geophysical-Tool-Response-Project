{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries to use\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import itertools\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV's as pandas DF variables\n",
    "tool_0028AA_df = pd.read_csv(\"Resources/Output_data/tool_0028AA_df.csv\")\n",
    "tool_9622C_df = pd.read_csv(\"Resources/Output_data/tool_9622C_df.csv\")\n",
    "# tool_xxx_df = pd.read_csv(\"Resources/xxx.csv\")\n",
    "# tool_xxx_df = pd.read_csv(\"Resources/xxx.csv\")\n",
    "# tool_xxx_df = pd.read_csv(\"Resources/xxx.csv\")\n",
    "# tool_xxx_df = pd.read_csv(\"Resources/xxx.csv\")\n",
    "# tool_xxx_df = pd.read_csv(\"Resources/xxx.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_0028AA_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_9622C_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_9622C_df.head()\n",
    "# 'SUSCEP_CGS E-5' 'SANGB_DEG' 'TEMP_CPS' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the datasets together based on well and Depth_M\n",
    "merged_df = pd.merge(tool_0028AA_df, tool_9622C_df, on=['well', 'Depth_M'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE LATER\n",
    "merged_df = merged_df.sample(n=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for a single well prefix RH\n",
    "prefix_counts = merged_df['well'].str.extract('^(.*?)\\d+')[0].value_counts()\n",
    "prefix_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix_counts = tool_0028A_df['well'].str.extract('^(.*?)\\d+')[0].value_counts()\n",
    "# prefix_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = merged_df[merged_df['well'].str.startswith('RHRC')]\n",
    "filtered_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "merged_cleaned_df = filtered_df[['well', 'Depth_M', 'SUSCEP_CGS E-5', 'DENSITY_G/CC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean nulls\n",
    "merged_data = merged_cleaned_df.replace(-999.25, pd.NA)\n",
    "merged_data.dropna(inplace=True)\n",
    "merged_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior to scaling, obviously invalid data such as negative Susceptability readings and datapoints that fall well outside the tools expected rages will be removed\n",
    "# filter out rows where Susceptability falls below the tools specified minimum value using tool documentation https://www.century-geo.com/9622\n",
    "merged_filtered_data = merged_data[\n",
    "    (merged_data[\"SUSCEP_CGS E-5\"] >= 0) &\n",
    "    (merged_data[\"DENSITY_G/CC\"] >= 0) &\n",
    "    (merged_data[\"DENSITY_G/CC\"] <= 6) &\n",
    "    (merged_data[\"SUSCEP_CGS E-5\"] <= 35)\n",
    "].copy()\n",
    "\n",
    "merged_filtered_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows that contain outliers that fall outside 3 standard deviations for the columns in order to create clusters on legitimate datapoints\n",
    "# Calculate the mean and standard deviation for each column\n",
    "mean_values = merged_filtered_data.mean()\n",
    "std_values = merged_filtered_data.std()\n",
    "\n",
    "# Define the upper and lower bounds for filtering\n",
    "lower_bounds = mean_values - (3 * std_values)\n",
    "upper_bounds = mean_values + (3 * std_values)\n",
    "merged_filtered_data = merged_filtered_data[\n",
    "    (merged_filtered_data['SUSCEP_CGS E-5'] >= lower_bounds['SUSCEP_CGS E-5']) &\n",
    "    (merged_filtered_data['SUSCEP_CGS E-5'] <= upper_bounds['SUSCEP_CGS E-5']) &\n",
    "    (merged_filtered_data['DENSITY_G/CC'] >= lower_bounds['DENSITY_G/CC']) &\n",
    "    (merged_filtered_data['DENSITY_G/CC'] <= upper_bounds['DENSITY_G/CC']) #&\n",
    "    #(merged_filtered_data['TEMP_CPS'] >= lower_bounds['TEMP_CPS']) &\n",
    "    #(merged_filtered_data['TEMP_CPS'] <= upper_bounds['TEMP_CPS'])\n",
    "]\n",
    "\n",
    "merged_filtered_data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict clusters using Empirical Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the filtered Data Remove data that is not to be scaled (Well and Depth)\n",
    "scaled_merged_filtered_data = merged_filtered_data.drop(columns=['well', 'Depth_M'])\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(scaled_merged_filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose number of clusters\n",
    "k = 4\n",
    "mahalanobis_distances = []\n",
    "\n",
    "model = KMeans(n_clusters=k)\n",
    "model.fit(scaled_data)\n",
    "scaled_merged_df = pd.DataFrame(scaled_data, columns=['SUSCEP_CGS E-5', 'DENSITY_G/CC'])\n",
    "scaled_merged_df['cluster_label'] = model.labels_\n",
    "\n",
    "for i in range(k):\n",
    "    cluster_points = scaled_merged_df.loc[scaled_merged_df['cluster_label'] == i, ['SUSCEP_CGS E-5', 'DENSITY_G/CC']]\n",
    "    cov = EmpiricalCovariance().fit(cluster_points)\n",
    "    cluster_center_reshaped = np.reshape(model.cluster_centers_[i], (1, -1))\n",
    "    mahalanobis_dist = distance.cdist(cluster_points, cluster_center_reshaped, 'mahalanobis', VI=cov.covariance_)\n",
    "    mahalanobis_distances.extend(mahalanobis_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_merged_df['min_mahalanobis_distance'] = np.min(mahalanobis_distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine outliers based on a threshold (e.g., 3 standard deviations from the mean) CAN BE CHANGED DEPENDING ON OUTPUT\n",
    "threshold = np.mean(scaled_merged_df['min_mahalanobis_distance']) + 3 * np.std(scaled_merged_df['min_mahalanobis_distance'])\n",
    "scaled_merged_df['is_outlier'] = scaled_merged_df['min_mahalanobis_distance'] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the DataFrame\n",
    "scaled_merged_df.hvplot.scatter(\n",
    "    x=\"SUSCEP_CGS E-5\",\n",
    "    y=\"DENSITY_G/CC\",\n",
    "    color=\"cluster_label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the DataFrame\n",
    "scaled_merged_df.hvplot.scatter(\n",
    "    x=\"SUSCEP_CGS E-5\",\n",
    "    y=\"DENSITY_G/CC\",\n",
    "    color=\"is_outlier\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict outliers using Elliptic Enveliope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose number of clusters\n",
    "k = 3\n",
    "mahalanobis_distances = np.empty((len(scaled_merged_df),))  # Initialize an empty NumPy array\n",
    "\n",
    "model = KMeans(n_clusters=k)\n",
    "model.fit(scaled_data)\n",
    "scaled_merged_df = pd.DataFrame(scaled_data, columns=['SUSCEP_CGS E-5', 'DENSITY_G/CC'])\n",
    "scaled_merged_df['cluster_label'] = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_label in np.unique(scaled_merged_df['cluster_label']):\n",
    "    cluster_data = scaled_merged_df.loc[scaled_merged_df['cluster_label'] == cluster_label, [\"DENSITY_G/CC\", \"SUSCEP_CGS E-5\"]]  # Adjust the features accordingly\n",
    "\n",
    "    # Fit the Elliptic Envelope on the cluster data\n",
    "    envelope = EllipticEnvelope()\n",
    "    envelope.fit(cluster_data)\n",
    "\n",
    "    # Calculate the Mahalanobis distance for each data point in the cluster\n",
    "    cluster_distances = envelope.mahalanobis(cluster_data)\n",
    "\n",
    "    # Assign the Mahalanobis distances to the corresponding indices in the mahalanobis_distances array\n",
    "    mahalanobis_distances[scaled_merged_df['cluster_label'] == cluster_label] = cluster_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average Mahalanobis distance across all clusters\n",
    "scaled_merged_df['Mahalanobis_Distance'] = mahalanobis_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold to determine outliers\n",
    "threshold = 8  # Adjust as needed\n",
    "\n",
    "# Identify outliers based on the threshold\n",
    "scaled_merged_df['is_outlier'] = scaled_merged_df['Mahalanobis_Distance'] > threshold\n",
    "scaled_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outliers with data to see if it is classifying properly\n",
    "scaled_merged_df.hvplot.scatter(\n",
    "    x=\"SUSCEP_CGS E-5\", \n",
    "    y=\"DENSITY_G/CC\", \n",
    "    by=\"cluster_label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outliers with data to see if it is classifying properly\n",
    "scaled_merged_df.hvplot.scatter(\n",
    "    x=\"SUSCEP_CGS E-5\", \n",
    "    y=\"DENSITY_G/CC\", \n",
    "    by=\"is_outlier\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCSVLogger(CSVLogger):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 == 0:\n",
    "            super().on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning Parameters\n",
    "activation_functions = ['relu', 'sigmoid', 'tanh']\n",
    "hidden_nodes_layer1_values = [32, 64, 128]\n",
    "hidden_nodes_layer2_values = [16, 32, 64]\n",
    "optimizers = ['adam', 'rmsprop']\n",
    "losses = ['binary_crossentropy', 'mean_squared_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_combinations = list(itertools.product(activation_functions, hidden_nodes_layer1_values, hidden_nodes_layer2_values, optimizers, losses))\n",
    "parameter_combinations[103]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(activation, hidden_nodes_layer1, hidden_nodes_layer2, optimizer, loss):\n",
    "    nn = tf.keras.models.Sequential()\n",
    "    nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=activation))\n",
    "    nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=activation))\n",
    "    nn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "    nn.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    nn.fit(X_train_scaled, y_train, epochs=20, callbacks=[CustomCSVLogger('model_tuning_results.csv', append=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=['Activation', 'Hidden Nodes Layer 1', 'Hidden Nodes Layer 2', 'Optimizer', 'Loss'])\n",
    "for params in parameter_combinations:\n",
    "    activation, hidden_nodes_layer1, hidden_nodes_layer2, optimizer, loss = params\n",
    "    train_model(activation, hidden_nodes_layer1, hidden_nodes_layer2, optimizer, loss)\n",
    "    results_df = results_df.append({'Activation': activation,\n",
    "                                    'Hidden Nodes Layer 1': hidden_nodes_layer1,\n",
    "                                    'Hidden Nodes Layer 2': hidden_nodes_layer2,\n",
    "                                    'Optimizer': optimizer,\n",
    "                                    'Loss': loss}, ignore_index=True)\n",
    "    \n",
    "results_df.to_csv('model_parameters.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
